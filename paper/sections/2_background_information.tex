% Background Information
\section{Background Information}

\subsection{Co-optimizing of morphology and control}
    \subsubsection{Embodied cognition}
        Despite the increase of computing power and knowledge, significant progress in the co-evolution of MC-pairs has remained stagnant. Researchers have proposed various hypotheses causing this, ranging from deficiencies in optimization algorithms to the notion that the training environments are not complex enough to facilitate morphological evolution, as outlined by Cheney et al. \cite{Cheney_2016}. However, they suggest a new hypotheses to explain the difficulty in co-evolving MC-pairs, which is the theory of embodied cognition. 

        Embodied cognition suggests that the cognitive processes arise not solely from the brain, but are a product of the dynamic interaction between the brain and morphology. This interplay means that even minor changes in morphology can disrupt the connection between the controller and morphology, requiring the controller to adapt itself again to the new morphology. This is also the reason why in \cite{Cheney_2016} the morphologies generally converge before the 100th generation out of a total of 5000 generations, because at that point, further morphological evolution becomes less beneficial as it will only result in lower fitness scores. This phenomenon of premature convergence poses a challenge in co-evolving MC-pairs.
 
    \subsubsection{Premature convergence}
        Premature convergence occurs when the population quickly converges to a local optimum, resulting in a lack of genetic diversity and suboptimal solutions. In the co-evolution of MC-pairs, this leads to early stagnation of morphology evolution, as morphological changes disrupt optimized controllers and are being discarded by selection pressure, preventing the benefits of co-evolution \cite{Luis_2024}.
        
        For the co-evolution of MC-pairs, Lehman et al. \cite{Lehman_2011} propose the Novelty Search with Local Competition (NSLC) algorithm, which utilizes a multi-objective search to optimize both diverse morphologies and fitness in conjunction with local competition, rewarding agents that outperform others with similar morphologies. While this approach did show an increase in the novelty of morphology, it did not yield higher fitness scores compared to using only a fitness objective function. Another method to address premature convergence is fitness sharing, which modifies the fitness function so that agents that are similar get penalized, thereby preserving and encouraging more morphological diversity \cite{McKay_2000}. Subsequent studies by Cheney et al. \cite{Cheney_2017} propose explicitly protecting agents that have undergone a recent morphological mutation, which reduces the selection pressure and gives the controller more time to adapt to the new morphology. Using the open-source soft-body simulator VoxCad as the physics engine, their experiments showed that this method produced significantly higher fitness scores and increased morphological diversity. Additionally, it delayed premature convergence, as the best-performing agents emerged in later generations, who initially would have been discarded otherwise. Stensby et al. \cite{Emma_Stensby_2021} extended upon this work by utilizing a more indirect approach to mitigate premature convergence. They discuss that increasing the agents' exploration of new morphologies can be facilitated by training the agents in a diverse range of environments. These environments were made incrementally more challenging using the Paired Open-Ended Trailblazer (POET) algorithm. Their findings demonstrated that environments generated by POET increased morphology diversity, indicating that POET, or other forms of curriculum training, could be effective in delaying convergence.

\subsection{Evolutionary strategies}
    \subsubsection{Neuroevolution}
        Neuroevolution, the process of evolving artificial neural networks through evolutionary algorithms, has been proven to be a great alternative to traditional reinforcement learning approaches, particularly suited for tasks with continuous and high-dimensional input spaces \cite{Pagliuca_2020}. This method begins by generating an initial population, and then selects the best solutions based on an evaluation function to produce the next generation of solutions. Evolutionary algorithms can be subdivided into genetic algorithms and evolutionary strategies. Genetic algorithms apply principles of evolution, such as selection, mutation, and recombination, typically encoding the parameters into binary strings. In contrast, evolutionary strategies utilize solely mutation and do not encode the parameters \cite{Dianati_2002}. One of the challenges with these approaches is that they require an extensive number of evaluations due to the size of the population. Nonetheless, it is sometimes feasible to substitute the original evaluation function with a less accurate one, but lower in computational costs \cite{Echevarrieta_2024}.

    \subsubsection{CPPN-NEAT}
        Conventionally, the topology of the neural network was established and fixed prior to training, which is a huge drawback, because the network's topology significantly influences the agent's performance. This drawback led to the development of Topology and Weight Evolving Artificial Neural Networks (TWEANNs), with the most prominent being the NeuroEvolution of Augmenting Topologies (NEAT) algorithm \cite{Stanley_2002}. This algorithm evolves both the weights and the topology of neural networks. It initializes a population of minimal networks with only input and output nodes and incrementally adds nodes and connections through mutations while utilizing a genetic algorithm for optimization. An important technique NEAT uses is speciation, which protects mutated networks, ensuring that new structures are not discarded and have the chance to evolve.

        A notable advantage of NEAT is its use in conjunction with Compositional Pattern-Producing Networks (CPPNs). Unlike conventional neural networks, CPPNs utilize a variety of activation functions to generate complex and regular patterns \cite{Stanley_2007}. This can be employed for encoding morphology parameters by mapping morphology features to specific values. In a study by Cheney et al. \cite{Cheney_2014}, CPPN-NEAT was used to encode the morphology of a soft robot, demonstrating that generative encoding using CPPNs assigned tissue cells to the voxels in a logical way, ensuring global coordination, resulting in improved locomotion. In contrast, the direct encoding assigned the voxels more independently of their neighboring voxels, resulting in less coordination and worsening locomotion.

    \subsubsection{Constraint-handling}
        Many optimization situations are subject to inherent constraints, such as the angular limits of a robotic arm, the allowed range of values for a mathematical function or simply in real life to use as less of a resource as possible. To effectively adhere to these constraints, evolutionary algorithms can employ several methods that direct the search away from prohibited areas of the solution space. For instance, penalty functions, which reduce the fitness score of solutions that violate constraints, therefore discouraging their selection. Additionally, repair algorithms actively modify solutions to meet the constraints that are in place. Another method is the multiobjective approach, which treats each constraint as a separate objective to be minimized alongside the main objective \cite{Kramer_2010}.



\subsection{Robustness and generalizability}
    Robustness refers to the ability of an agent to maintain desirable behavior despite variations or perturbations in its input and output data \cite{Ravi_Mangal_2019, Charles_Packer_2019, Xu_Mengdi_2022}. Consequently, a robust agent is less susceptible to input and output perturbations. This makes robustness a critical attribute, as inputs and outputs from the training environment can differ significantly from those in the testing environment, especially in real-world scenarios. For instance, an input perturbation can be caused by a sensor defect, resulting in slight measurement errors. In reinforcement learning, this means that we need to build robustness against the uncertainty of state observations and the actual state. Similarly, an output perturbation can result from a motory issue of the agent. In reinforcement learning, this means that we need to build robustness against uncertain actions between the actions generated by the agent and the conducted actions \cite{Xu_Mengdi_2022}. 

    On the other hand, generalizability refers to the ability of an agent to maintain desirable behavior under different conditions to those encountered during training. It assumes correct input and output data and is concerned on the actual differences between the training environment and the testing or production environment. Testing generalization is divided into two distinctive parts. The first part is called interpolation, which requires the agent to perform well in environments similar to those of training, with both the testing and training environment parameters drawn from the same distribution. The second part is called extrapolation, which requires the agent to perform well in environments different from those of training, with the testing and training environment parameters drawn from separate distributions \cite{Charles_Packer_2019, Xu_Mengdi_2022}.

    There are some ways to achieve more robust agents. In this example \cite{Ines_Valentin_2022}, they compared the robustness of the models DENSER and NSGA-Net on the CIFAR-10 image classification task. It was found that the DENSER model exhibited an higher robustness, which concludes that certain models inherently have a higher degree of exhibiting robustness. Furthermore, one of the most popular method is adverserial training, where the agent is trained on adverserially perturbated training data. One way of doing this is finding the worst case perturbation at each training episode and training the model using the dataset with this perturbation \cite{Kai_Liang_Tan_2020}. 
    
    The two main approaches to achieving generalizable agents are either training a generalist agent, which involves a trade-off in performance under specific conditions compared to a specialist agent, as shown by Triebold et al. \cite{Corinna_Triebold}, or developing agents that can explicitly adapt to certain conditions \cite{Charles_Packer_2019}. In this paper, we will focus on the first approach. Recently, there has been a growing recognition of the importance of using variability to train better and more generalized agents. Raviv et al. \cite{Limor_Raviv_2022} discusses the relationship between variability and learning outcomes, highlighting a universal principle that variability enhances learning. This principle also holds true for machine learning, where employing a more variable learning schedule can improve an agent's generalizability. Similarly, Stensby et al. \cite{Emma_Stensby_2021}, applied a similar principle of variability by training agents in a curriculum of environments generated by the Paired Open-Ended Trailblazer (POET) algorithm, where the generated environments were incrementally more difficult, enabling the agent to learn more efficient and complex behaviors. These studies demonstrate that using a variable learning schedule increases both robustness and generalizability.