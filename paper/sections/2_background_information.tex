% Background Information
\section{Background Information}

\subsection{Co-optimizing of morphology and control}
    \subsubsection{Embodied cognition}
        Despite the increase of computing power and knowledge, there has not been a significant leap forward since Sims' initial work on the co-optimization of morphology-controller pairs. Researchers have proposed various hypotheses causing this, ranging from deficiencies in optimization algorithms to the notion that the training environments are not complex enough to facilitate morphological evolution, as outlined by Cheney et al. \cite{Nick_Cheney_2016}. However, they suggest a new hypotheses to explain the difficulty in co-optimizing morphology-controller pairs, which is the theory of embodied cognition. 

        \textbf{(Put more elaboration on what embodied cognition is)} Embodied cognition suggests that the cognitive processes arise not solely from the brain, but are a product of the dynamic interaction between the brain and morphology. This interplay means that even minor changes in morphology can disrupt the connection between the controller and morphology, requiring the controller to adapt itself again to the new morphology. This is also the reason why in \cite{Nick_Cheney_2016} the morphologies generally converge before the 100th generation out of a total of 5000 generations, because at that point, further morphological evolution becomes less beneficial as it will only result in lower fitness scores. This phenomenon of premature convergence poses a challenge in co-optimizing morphology-controller pairs.
 
    \subsubsection{Premature convergence}
        Premature convergence occurs when the population quickly converges to a local optimum, resulting in a lack of genetic diversity and suboptimal solutions. In the co-evolution of robot morphology and controllers, this leads to early stagnation of morphology evolution, as morphological changes disrupt optimized controllers and are discarded by selection pressure, preventing the benefits of co-evolution. \cite{Morret_2024}
        
        For the co-optimization of morphology-controller pairs, Lehman et al. \cite{Lehman_2011} propose the Novelty Search with Local Competition (NSLC) algorithm, which utilizes a multi-objective search to optimize both diverse morphologies and fitness in conjunction with local competition, rewarding agents that outperform others with similar morphologies. While this approach did show an increase in the novelty of morphology, it did not yield higher fitness scores compared to using only a fitness objective function. Another method to address premature convergence is fitness sharing, which modifies the fitness function so that agents that are similar get penalized, thereby preserving and encouraging more morphological diversity \cite{McKay_2000}. Subsequent studies by Cheney et al. \cite{Nick_Cheney_2017} propose explicitly protecting agents that have undergone a recent morphological mutation, which reduces the selection pressure and gives the controller more time to adapt to the new morphology. Using the open-source soft-body simulator VoxCad as the physics engine, their experiments showed that this method produced significantly higher fitness scores and increased morphological diversity. Additionally, it delayed premature convergence, as the best-performing agents emerged in later generations which would have been discarded otherwise.

\subsection{Robustness and generalizability}
Robustness refers to the ability of an agent to maintain desirable behavior despite variations or perturbations in its input and output data \cite{Ravi_Mangal_2019, Charles_Packer_2019, Xu_Mengdi_2022}. Consequently, a robust agent is less susceptible to input and output perturbations. This makes robustness a critical attribute, as inputs and outputs from the training environment can differ significantly from those in the testing environment, especially in real-world scenarios. For instance, an input perturbation can be caused by a sensor defect, resulting in slight measurement errors. In reinforcement learning, this means that we need to build robustness against the uncertainty of state observations and the actual state. Similarly, an output perturbation can result from a motory issue of the agent. In reinforcement learning, this means that we need to build robustness against uncertain actions between the actions generated by the agent and the conducted actions \cite{Xu_Mengdi_2022}. 

On the other hand, generalizability refers to the ability of an agent to maintain desirable behavior under different conditions to those encountered during training. It assumes correct input and output data and is concerned on the actual differences between the training environment and the testing or production environment. Testing generalization is divided into two distinctive parts. The first part is called interpolation, which requires the agent to perform well in environments similar to those of training, with both the testing and training environment parameters drawn from the same distribution. The second part is called extrapolation, which requires the agent to perform well in environments different from those of training, with the testing and training environment parameters drawn from separate distributions \cite{Charles_Packer_2019, Xu_Mengdi_2022}.

There are some ways to achieve more robust agents. In this example \cite{Ines_Valentin_2022}, they compared the robustness of the models DENSER and NSGA-Net on the CIFAR-10 image classification task. It was found that the DENSER model exhibited an higher robustness, which concludes that certain architectures inherently have a higher degree of exhibiting robustness. Furthermore, one of the most popular method is adverserial training, where the agent is trained on adverserially perturbated training data. One way of doing this is finding the worst case perturbation at each training episode and training the model using the dataset with this perturbation \cite{Kai_Liang_Tan_2020}. The two main approaches to achieving generalizable agents are either training a generalist agent, which involves a trade-off in performance under specific conditions compared to a specialist agent, as shown by Triebold et al. \cite{Corinna_Triebold}, or developing agents that can explicitly adapt to certain conditions \cite{Charles_Packer_2019}. In this paper, we will focus on the first approach. Recently, there has been a growing recognition of the importance of using variability to train better and more generalized agents. Raviv et al. \cite{Limor_Raviv_2022} discusses the relationship between variability and learning outcomes, highlighting a universal principle that variability enhances learning. This principle also holds true for machine learning, where employing a more variable learning curriculum can improve an agent's generalizability across a wide range of morphologies. Similarly, Stensby et al. \cite{Emma_Stensby_2021}, applied the same principle of variability by training agents in a curriculum of environments generated by the Paired Open-Ended Trailblazer (POET) algorithm, where the generated environments were incrementally more difficult, enabling the agent to learn more efficient and complex behaviors. These studies demonstrate that using a variable learning curriculum increases both robustness and generalizability.